# Malware-Lookup-Service

### Problem Statement:

We have an HTTP proxy that is scanning traffic, looking for malware URLs. Before allowing HTTP connections to be made, this proxy asks a service that maintains several databases of malware URLs if the resource being requested is known to contain malware. Write a small web service, that responds to GET requests where the caller passes in a URL and the service responds with some information about that URL. The GET requests would look like this:
GET /v1/urlinfo/{resource_url_with_query_string}

### Current Architecture:

```

   +-------------+        +-------------+  (2)   +----------------+  (3)   +-------------+ 
   |             |   (1)  |             | -----> |                | -----> |             |
   |    USER     | -----> |    PROXY    |        | WEB LOOKUP API |        |   DATABASE  |
   |             |        |             | <----- |                | <----- |             |
   +-------------+        +-------------+        +----------------+        +-------------+ 
                                 |
                                 | (4)
                                 |
                                \_/
                          +-------------+
                          |   WEBSITE   |
                          +-------------+
```
   
1. User connects to Proxy while accessing any url
2. Proxy send a GET REQUEST to the API with the url encoded
3. API verifies its status within database and returns classification
4. Proxy allows access to website if not malicious

### API initialisation and testing:

1.  Clone this git repository onto your local machine
2.	Create a python virtual environemnt and pip install -r requirements.txt
3.  Choose either the simple database lookup or machine learnning approach(that can be integrated with the database approach in the future):

#### Database Lookup:

i. Initialize the sqlite database using db.create_all() within your python shell or while uncommenting the line from app.py. Subsequent runs do not need this line since it would override the data in the database and reinitialize

ii. Keep the flask app running in a separate terminal

iii. Add a few sample entries to your database from the test.py PUT method, along with their classification, on a separate terminal

iv. Test out whether the url submitted is malware or safe using the GET method on a separate terminal

##### Machine learning(Future Enhancement):

i. Run the machine_learning.py file and make sure that the data utilized is in an csv format, exists in the same folder, and that the system is able to read it

ii. Test it using websites from both in and out of the database and check accuracy

iii. If the accuracy is  permissible, we can consider adding this functionality to the database lookup approach as well

#### Future Enhancements and Planned Architecture:

1. The url lookup service is very simplistic as of now as it only returns if a website is safe or not. There is a lot of room for further improvements, for example allow parsing of the ur using urllib to store particular domains, protocols, extensions, and more as safe - barring which the user will have to request access before it can be added to the database.
2. Including the use of sessions cookies or cached data of some form in order to avoid repeated checking within the database
3. Updater in case a multi-threaded approach is utilized in the future

### Additional Considerations:
● The size of the URL list could grow infinitely, how might you scale this beyond the memory capacity of the system?

Sharding the resource_url, perhaps by employing a redis cluster for data persistsence and its ability to allow for sharding while scaling

● The number of requests may exceed the capacity of this system, how might you solve that?

We could add a load balancer to our proxy to manage increasing number of requests and responses to the system, which would spawn additional nodes and employ multiple threads to assign each request with available workers

● What are some strategies you might use to update the service with new URLs? Updates may be as many as 5000 URLs a day with updates arriving every 10 minutes.

To scale beyond the simple POST request implementation that is currently present, we could store data on multiple servers(with a predetermnined classification, such as by domain first letter, type or any such discriminator) and parse/access the data according to this classification. Of course, we would also have to keep the CAP theorem in mind while having multi-threaded processes updating and accessing from a given database at any point of time
